---
jupyter:
  jupytext:
    text_representation:
      extension: .md
      format_name: markdown
      format_version: '1.3'
      jupytext_version: 1.17.3
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
---

<!-- #region _cell_guid="ece4f2d9-bf7b-40d2-923d-8e5a72ebfafe" _uuid="6ad05b75-aabe-427b-84a8-a93ae58b9672" jupyter={"outputs_hidden": false} papermill={"duration": 0.004666, "end_time": "2025-07-29T21:22:31.509832", "exception": false, "start_time": "2025-07-29T21:22:31.505166", "status": "completed"} -->
 ## 1. Load the mdoels, show example of the inference
<!-- #endregion -->

```python _cell_guid="79baad5b-9002-47a1-9b4c-32e937af78e1" _uuid="db36416e-e23e-4261-8d7d-844f578bf7bf" jupyter={"outputs_hidden": false} papermill={"duration": 89.146296, "end_time": "2025-07-29T21:24:00.660087", "exception": false, "start_time": "2025-07-29T21:22:31.513791", "status": "completed"}
pip install ultralytics rfdetr
```

```python _cell_guid="0414d3b3-2803-4f86-b240-b1db9bff6aea" _uuid="f7b6e671-1a6e-4d38-958e-d4c232246a6f" jupyter={"outputs_hidden": false} papermill={"duration": 0.032818, "end_time": "2025-07-29T21:24:00.719578", "exception": false, "start_time": "2025-07-29T21:24:00.686760", "status": "completed"}
import sys
sys.path.append("/kaggle/input/dpm3-src")
```

```python papermill={"duration": 0.031137, "end_time": "2025-07-29T21:24:00.777031", "exception": false, "start_time": "2025-07-29T21:24:00.745894", "status": "completed"}
RUN_FAST_DEV = False
```

```python _cell_guid="367f7fb2-554f-418d-ab1b-126b17aeded2" _uuid="a559193a-fb11-45f4-af87-80793587175d" jupyter={"outputs_hidden": false} papermill={"duration": 14.450702, "end_time": "2025-07-29T21:24:15.253759", "exception": false, "start_time": "2025-07-29T21:24:00.803057", "status": "completed"}
from src.odc.benchmark import (
    UltralyticsModel,
    FasterRCNNModel,
    RFDETRModel,
    SkyFusionDataset,
)
```

```python _cell_guid="e3150ace-2565-4d3b-bbc5-93cf6726f4ab" _uuid="5dfd9464-2312-4b2c-8d7b-5f0ef17508d4" jupyter={"outputs_hidden": false} papermill={"duration": 0.033615, "end_time": "2025-07-29T21:24:15.366103", "exception": false, "start_time": "2025-07-29T21:24:15.332488", "status": "completed"}
models_config = [
    {
        "name": "yolov8m-aug-update_20250603",
        "path": "/kaggle/input/dpm3-models/models/yolov8m-aug-update_20250603.pt",
        "params": {
            "conf_thr": 0.06291186979342091,
            "nms_iou": 0.3358295483691517,
        },
        "type": "yolo",
    },
    {
        "name": "yolov11m-p2-aug_20250603",
        "path": "/kaggle/input/dpm3-models/models/yolov11m-p2-aug_20250603.pt",
        "params": {
            "conf_thr": 0.052566007120515956,
            "nms_iou": 0.49317179138811856,
        },
        "type": "yolo",
    },
    {
        "name": "rf-detr",
        "path": "/kaggle/input/dpm3-models/models/rfdetr_best_total.pth",
        "params": {
            "conf_thr": 0.09616820140192325,
        },
        "type": "rfdetr",
    },
    {
        "name": "faster-rcnn",
        "path": "/kaggle/input/dpm3-models/models/fasterrcnn-best-epoch=18-val_map=0.31.ckpt",
        "params": {
            "conf_thr": 0.07957236023833904,
            "nms_iou": 0.621230971215935,
        },
        "type": "faster-rcnn",
    },
    {
        "name": "rt-detr",
        "path": "/kaggle/input/dpm3-models/models/rtdetr-aug_best.pt",
        "params": {
            "conf_thr": 0.2704984199324548,
        },
        "type": "rtdetr",
    },
]
```

```python _cell_guid="cdd51657-fb04-48e2-b007-3fa3cdff6fe7" _uuid="8def9ddc-73ce-442a-b81c-c300fc00572c" jupyter={"outputs_hidden": false} papermill={"duration": 0.033436, "end_time": "2025-07-29T21:24:15.425473", "exception": false, "start_time": "2025-07-29T21:24:15.392037", "status": "completed"}
from yaml import safe_load
from typing import Iterator, Any
from copy import deepcopy
import json
import os
from datetime import datetime


def all_model_loader(configs: list[dict[str, Any]], config = None) -> Iterator[tuple[str, Any]]:
    for model_info in configs:
        model_type = model_info["type"]
        name = model_info["name"]
        path = model_info["path"]

        if model_type == "rtdetr":
            yield name, UltralyticsModel(path, config)
        elif model_type == "faster-rcnn":
            yield name, FasterRCNNModel(path, config)
        elif model_type == "yolo":
            yield name, UltralyticsModel(path, config)
        elif model_type == "rfdetr":
            yield name, RFDETRModel(path, config)
        else:
            raise ValueError(f"Unknown model type: {model_type}")
```

```python _cell_guid="94e492e4-9545-446f-bd80-5ea5b1eb1419" _uuid="1610d0bc-d1d3-4360-8b15-47a6c4563343" jupyter={"outputs_hidden": false} papermill={"duration": 0.033751, "end_time": "2025-07-29T21:24:15.485462", "exception": false, "start_time": "2025-07-29T21:24:15.451711", "status": "completed"}
models_config
```

```python _cell_guid="f7385b49-4a88-4325-ad52-147dca6eb51b" _uuid="a77b6d79-9ac7-4cbd-b584-ee908c26af3e" jupyter={"outputs_hidden": false} papermill={"duration": 6.698451, "end_time": "2025-07-29T21:24:22.210221", "exception": false, "start_time": "2025-07-29T21:24:15.511770", "status": "completed"}
DATA_DIR = "/kaggle/input/skyfusion-yolo-v8/SkyFusion_yolo"
dataset = SkyFusionDataset(DATA_DIR, config=dict(), split="test")
dataset = dataset[:5] if RUN_FAST_DEV else dataset
```

```python _cell_guid="891ae3a3-45eb-4291-9100-9c4272d8e7f9" _uuid="d46c53dc-3082-4e3f-a8e9-e1c78e4ec847" jupyter={"outputs_hidden": false} papermill={"duration": 0.040219, "end_time": "2025-07-29T21:24:22.281302", "exception": false, "start_time": "2025-07-29T21:24:22.241083", "status": "completed"}
import cv2
import numpy as np
import matplotlib.pyplot as plt
from typing import List, Dict, Any


def draw_boxes(image_bgr: np.ndarray, boxes: List[object], color: tuple, show_confidence: bool = False) -> np.ndarray:
    """
    Draws bounding boxes on an image.

    Args:
        image_bgr (np.ndarray): Image in BGR format.
        boxes (List[object]): List of annotations or detections (must have bbox and class_name attributes).
        color (tuple): Color for the bounding boxes (B, G, R).
        show_confidence (bool): If True, includes confidence in the label.

    Returns:
        np.ndarray: Annotated image.
    """
    output_image = image_bgr.copy()

    for box in boxes:
        x1, y1, x2, y2 = map(int, box.bbox)
        label = box.class_name
        if show_confidence and hasattr(box, 'confidence'):
            label += f" {box.confidence:.2f}"

        cv2.rectangle(output_image, (x1, y1), (x2, y2), color, 2)
        cv2.putText(output_image, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)

    return output_image


def compare_model_predictions(sample: Any, configs: list[dict[str, Any]]) -> None:
    image: np.ndarray = sample.image
    ground_truth_annotations = sample.annotations
    h, w, _ = image.shape

    for model_name, model in all_model_loader(models):
        predictions = model.predict(image)

        image_with_gt = draw_boxes(image, ground_truth_annotations, color=(255, 0, 0))
        image_with_preds = draw_boxes(image, predictions, color=(0, 255, 0), show_confidence=True)

        comparison_image = np.hstack((image_with_gt, image_with_preds))

        label_bar = np.zeros((50, w * 2, 3), dtype=np.uint8)
        cv2.putText(label_bar, "Ground Truth (Blue)", (10, 35), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
        cv2.putText(label_bar, f"'{model_name}' Predictions (Green)", (w + 10, 35), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)

        final_image_bgr = np.vstack((label_bar, comparison_image))
        final_image_rgb = cv2.cvtColor(final_image_bgr, cv2.COLOR_BGR2RGB)

        plt.figure(figsize=(16, 10))
        plt.imshow(final_image_rgb)
        plt.axis('off')
        plt.title(f"Ground Truth vs. '{model_name}' Predictions", fontsize=16)
        plt.tight_layout()
        plt.show()


# # Assuming `dataset` and `models` are defined
# sample = dataset[3]
# compare_model_predictions(sample, models)
```

```python _cell_guid="43281508-5f22-46c2-82b5-cb958a9537a7" _uuid="6e78cab2-eb6f-4aa0-8237-cd88b8fd52da" jupyter={"outputs_hidden": false} papermill={"duration": 0.036242, "end_time": "2025-07-29T21:24:22.346928", "exception": false, "start_time": "2025-07-29T21:24:22.310686", "status": "completed"}
def instantiate_model_with_thresholds(model_info: Dict[str, Any], conf_thr: float, nms_thr: float):
    config = deepcopy(model_info.get("config", {}))
    config["conf_threshold"] = conf_thr
    config["iou_threshold"] = nms_thr

    model_type = model_info["type"]
    model_path = model_info["path"]

    if model_type in {"rtdetr", "yolo"}:
        return UltralyticsModel(model_path, config)
    elif model_type == "faster-rcnn":
        return FasterRCNNModel(model_path, config)
    elif model_type == "rfdetr":
        return RFDETRModel(model_path, config)
    else:
        raise ValueError(f"Unknown model type: {model_type}")
```

<!-- #region _cell_guid="58e3b483-7763-4a40-8acc-8957dfd418d5" _uuid="4a7af4aa-057e-4bd7-b6b0-20d89270949d" jupyter={"outputs_hidden": false} papermill={"duration": 0.028794, "end_time": "2025-07-29T21:24:22.404765", "exception": false, "start_time": "2025-07-29T21:24:22.375971", "status": "completed"} -->
 ## 2. TIDE Error Analysis
<!-- #endregion -->

```python _cell_guid="38e9da77-5c96-4a30-a108-da59eec808c1" _uuid="722c7c75-8006-4ab6-8501-74b34f245439" jupyter={"outputs_hidden": false} papermill={"duration": 3.627955, "end_time": "2025-07-29T21:24:26.061757", "exception": false, "start_time": "2025-07-29T21:24:22.433802", "status": "completed"}
# Install TIDE if not already installed
!pip install tidecv
```

```python _cell_guid="81bd0368-4f88-4b1b-97a0-eb02d401675c" _uuid="84f97d4b-554a-4acd-bbde-1dfcbd6ddc36" jupyter={"outputs_hidden": false} papermill={"duration": 0.0383, "end_time": "2025-07-29T21:24:26.129737", "exception": false, "start_time": "2025-07-29T21:24:26.091437", "status": "completed"}
def convert_predictions_to_coco_format(dataset, model, model_name: str, output_file: str = None):
    """
    Convert model predictions to COCO format for TIDE evaluation.
    
    Args:
        dataset: SkyFusionDataset instance
        model: Model instance for inference
        model_name: Name of the model for identification
        output_file: Optional path to save the results JSON file
    
    Returns:
        list: COCO-format predictions
    """
    predictions = []
    
    print(f"Generating predictions for {model_name}...")
    for idx in range(len(dataset)):
        if idx % 100 == 0:
            print(f"Processing image {idx}/{len(dataset)}")
            
        sample = dataset[idx]
        image = sample.image
        coco_image_id = idx  # Use index as COCO image_id
        
        # Get model predictions
        model_predictions = model.predict(image)
        
        # Convert to COCO format
        for pred in model_predictions:
            x1, y1, x2, y2 = pred.bbox
            width = x2 - x1
            height = y2 - y1
            
            # Ensure all values are native Python types (not numpy types)
            coco_pred = {
                "image_id": coco_image_id,
                "category_id": 1,  # Assuming single class (aircraft)
                "bbox": [float(x1), float(y1), float(width), float(height)],  # COCO format: [x, y, width, height]
                "score": float(pred.confidence if hasattr(pred, 'confidence') else 1.0)
            }
            predictions.append(coco_pred)
    
    if output_file:
        # Ensure we're writing valid JSON with proper encoding
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(predictions, f, indent=2, ensure_ascii=False)
        print(f"Predictions saved to {output_file} ({len(predictions)} detections)")
    
    return predictions
```

```python _cell_guid="77c4c09d-e5ea-4197-afb5-5164a9d8cfb9" _uuid="d1a4adfb-ae29-4a09-b30c-84c36ef80576" jupyter={"outputs_hidden": false} papermill={"duration": 0.03938, "end_time": "2025-07-29T21:24:26.198971", "exception": false, "start_time": "2025-07-29T21:24:26.159591", "status": "completed"}
def convert_ground_truth_to_coco_format(dataset, output_file: str = None):
    """
    Convert ground truth annotations to COCO format for TIDE evaluation.
    
    Args:
        dataset: SkyFusionDataset instance
        output_file: Optional path to save the annotations JSON file
    
    Returns:
        dict: COCO-format annotations
    """
    # COCO format structure
    coco_gt = {
        "images": [],
        "annotations": [],
        "categories": [
            {
                "id": 1,
                "name": "aircraft",
                "supercategory": "vehicle"
            }
        ]
    }
    
    annotation_id = 1
    
    print("Converting ground truth to COCO format...")
    for idx in range(len(dataset)):
        if idx % 100 == 0:
            print(f"Processing image {idx}/{len(dataset)}")
            
        sample = dataset[idx]
        image = sample.image
        original_image_id = sample.image_id if hasattr(sample, 'image_id') else f"image_{idx}"
        coco_image_id = idx  # Use index as COCO image_id
        h, w, _ = image.shape
        
        # Add image info
        image_info = {
            "id": coco_image_id,
            "width": int(w),
            "height": int(h),
            "file_name": original_image_id
        }
        coco_gt["images"].append(image_info)
        
        # Add annotations
        for ann in sample.annotations:
            x1, y1, x2, y2 = ann.bbox
            width = x2 - x1
            height = y2 - y1
            area = width * height
            
            # Ensure all values are native Python types (not numpy types)
            # Convert bbox to polygon format for TIDE compatibility
            x, y, w, h = float(x1), float(y1), float(width), float(height)
            polygon = [[x, y, x + w, y, x + w, y + h, x, y + h]]
            annotation = {
                "id": int(annotation_id),
                "image_id": coco_image_id,
                "category_id": 1,
                "bbox": [x, y, w, h],  # COCO format: [x, y, width, height]
                "area": float(area),
                "iscrowd": 0,
                "segmentation": polygon  # Polygon format for TIDE
            }
            coco_gt["annotations"].append(annotation)
            annotation_id += 1
    
    if output_file:
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(coco_gt, f, indent=2, ensure_ascii=False)
        print(f"Ground truth saved to {output_file} ({len(coco_gt['annotations'])} annotations)")
    
    return coco_gt
```

```python _cell_guid="e1bd5195-2377-42b5-a64c-f0bc2c88b65d" _uuid="a8ffa423-2c95-43b4-9246-cbc5696993d6" jupyter={"outputs_hidden": false} papermill={"duration": 0.038249, "end_time": "2025-07-29T21:24:26.266975", "exception": false, "start_time": "2025-07-29T21:24:26.228726", "status": "completed"}
def run_tide_evaluation(gt_file: str, pred_files: Dict[str, str], output_dir: str = "tide_results"):
    """
    Run TIDE evaluation on the predictions.
    
    Args:
        gt_file: Path to ground truth COCO annotations file
        pred_files: Dictionary mapping model names to prediction files
        output_dir: Directory to save TIDE results
    """
    from tidecv import TIDE
    import tidecv.datasets as datasets
    
    # Create output directory
    os.makedirs(output_dir, exist_ok=True)
    
    # Load ground truth
    print("Loading ground truth...")
    gt = datasets.COCO(gt_file)

    # Create TIDE object
    tide = TIDE()

    # Evaluate each model
    for model_name, pred_file in pred_files.items():
        print(f"Evaluating {model_name}...")

        # Pass file path directly to COCOResult
        pred_results = datasets.COCOResult(pred_file)
        
        # Run TIDE evaluation
        tide.evaluate_range(gt, pred_results, mode=TIDE.BOX, name=model_name)
    
    # Summarize results
    print("\n" + "="*50)
    print("TIDE EVALUATION SUMMARY")
    print("="*50)
    tide.summarize()
    
    # Plot results
    print("\nGenerating TIDE plots...")
    tide.plot()
    
    return tide
```

```python _cell_guid="9b46657e-e01b-4d5d-b7df-a55c41614390" _uuid="6e76bb2c-614d-4646-91b9-5f9defa3a31f" jupyter={"outputs_hidden": false} papermill={"duration": 0.426748, "end_time": "2025-07-29T21:24:26.723482", "exception": false, "start_time": "2025-07-29T21:24:26.296734", "status": "completed"}
# Generate COCO format files for TIDE evaluation

# Create output directory for TIDE files
tide_output_dir = "tide_evaluation"
os.makedirs(tide_output_dir, exist_ok=True)

# Convert ground truth to COCO format
gt_file = os.path.join(tide_output_dir, "ground_truth.json")
print("Converting ground truth to COCO format...")
coco_gt = convert_ground_truth_to_coco_format(dataset, gt_file)
```

```python _cell_guid="fe40edb8-e965-472c-ad13-781b765ce81c" _uuid="c3acfa21-d8bc-43e6-b27c-13a01651714c" jupyter={"outputs_hidden": false} papermill={"duration": 125.012275, "end_time": "2025-07-29T21:26:31.765682", "exception": false, "start_time": "2025-07-29T21:24:26.753407", "status": "completed"}
# Generate predictions for each model
pred_files = {}

for model_config in models_config:
    model_name = model_config["name"]
    print(f"\nProcessing {model_name}...")
    
    # Create model with optimized thresholds
    params = model_config["params"]
    conf_thr = params["conf_thr"]
    nms_iou = params.get("nms_iou", None)
    
    model = instantiate_model_with_thresholds(model_config, conf_thr, nms_iou)
    
    # Generate predictions
    pred_file = os.path.join(tide_output_dir, f"{model_name}_predictions.json")
    predictions = convert_predictions_to_coco_format(dataset, model, model_name, pred_file)
    pred_files[model_name] = pred_file
    
    print(f"Generated {len(predictions)} predictions for {model_name}")
```

```python _cell_guid="3a3b73e6-266d-4998-8822-028494c26746" _uuid="1e7d1d43-3a55-4667-9553-2ba6229ccf7b" jupyter={"outputs_hidden": false} papermill={"duration": 38.131546, "end_time": "2025-07-29T21:27:10.077002", "exception": false, "start_time": "2025-07-29T21:26:31.945456", "status": "completed"}
# Run TIDE evaluation
print("\nRunning TIDE evaluation...")
tide_results = run_tide_evaluation(gt_file, pred_files, tide_output_dir)
```

```python _cell_guid="70729617-6e6a-4f8f-a2b6-0ccaf3f03818" _uuid="b77ad013-51cb-44d7-8873-49d806c8def5" jupyter={"outputs_hidden": false} papermill={"duration": 0.196561, "end_time": "2025-07-29T21:27:10.463792", "exception": false, "start_time": "2025-07-29T21:27:10.267231", "status": "completed"}
tide_results
```

<!-- #region _cell_guid="15efa57e-3a3a-4054-8832-833f01a46c48" _uuid="5a94ddf2-95d2-4b46-86f4-9dc69205c8e3" jupyter={"outputs_hidden": false} papermill={"duration": 0.191547, "end_time": "2025-07-29T21:27:10.845513", "exception": false, "start_time": "2025-07-29T21:27:10.653966", "status": "completed"} -->
 ## 4. TIDE Results Analysis
 
 The TIDE evaluation provides detailed error analysis for object detection models:
 
 - **Cls**: Classification errors
 - **Loc**: Localization errors  
 - **Both**: Both classification and localization errors
 - **Dupe**: Duplicate detections
 - **Bkg**: Background (false positive) errors
 - **Miss**: Missed detections (false negatives)
 
 Use the plots and summary above to identify which error types are most prevalent for each model.
<!-- #endregion -->

```python _cell_guid="0c6055d4-0da0-4183-bd38-18413648a521" _uuid="c37de0c6-c32c-41d2-b411-59ab9a587a4a" jupyter={"outputs_hidden": false} papermill={"duration": 0.201857, "end_time": "2025-07-29T21:27:11.237529", "exception": false, "start_time": "2025-07-29T21:27:11.035672", "status": "completed"}
# Optional: Save detailed results to file
results_summary_file = os.path.join(tide_output_dir, "tide_summary.txt")
with open(results_summary_file, 'w') as f:
    f.write(f"TIDE Evaluation Results - {datetime.now()}\n")
    f.write("="*50 + "\n")
    f.write("Models evaluated:\n")
    for model_name in pred_files.keys():
        f.write(f"- {model_name}\n")
    f.write("\nFor detailed analysis, see the generated plots and console output above.\n")

print(f"\nTIDE evaluation complete! Results saved in '{tide_output_dir}' directory.")
print(f"Summary saved to: {results_summary_file}")
```
